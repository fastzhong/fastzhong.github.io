+++
title = "Kubernetes å®‰è£… - kubeadm"
date = 2020-09-13T03:08:47+08:00
readingTime = true
categories = ["äº‘ & äº‘åŸç”Ÿ"]
tags = ["kubernetes"]
toc = true
+++

å®˜æ–¹æ¨èé›†ç¾¤å·¥å…· kubeadm

<!--more-->

ğŸ’¡ å‚è€ƒï¼š<i class="fas fa-external-link-alt"></i>&nbsp;&nbsp;
[å®˜æ–¹æ–‡æ¡£](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/)

å•èŠ‚ç‚¹é›†ç¾¤æ¶æ„å¦‚ä¸‹å›¾ï¼š  
![k8s cluster](/images/k8s/single-master.jpg#center)

```md
-   K8s é›†ç¾¤ï¼šä¸€ä¸ªæ§åˆ¶èŠ‚ç‚¹ï¼ˆcontrol panel/masterï¼‰ï¼›ä¸¤ä¸ªè´Ÿè½½/å·¥ä½œèŠ‚ç‚¹ï¼ˆworkerï¼‰
-   Ubuntu xenial64
-   Kubernetes 1.19.1
-   Docker 5.19
```

| èŠ‚ç‚¹ï¼ˆè§’è‰²ï¼‰     | IP                     | ç»„ä»¶                                                                  |
| ---------------- | ---------------------- | --------------------------------------------------------------------- |
| k8s\-master1     | 192\.168\.100\.1       | kube\-apiserverï¼Œkube\-controller\-managerï¼Œkube\-schedulerï¼Œetcd     |
| ~~k8s\-master2~~ | ~~192\.168\.100\.1~~   | ~~kube\-apiserverï¼Œkube\-controller\-managerï¼Œkube\-schedulerï¼Œetcd~~ |
| k8s\-node1       | 192\.168\.100\.11      | kubeletï¼Œkube\-proxyï¼Œdocker                                          |
| k8s\-node2       | 192\.168\.100\.12      | kubeletï¼Œkube\-proxyï¼Œdocker                                          |
| ~~k8s\-lb~~      | ~~192\.168\.100\.100~~ | ~~Nginx, etcd~~                                                       |

## VM

VagrantFile:

```ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :

ENV["LC_ALL"] = "en_US.UTF-8"

# ---- è™šæœºé…ç½® ----
boxes = [
    {
        :name => "k8s-master1",
        :eth1 => "192.168.100.11",
        :mem => "2048",
        :cpu => "2"
    },
    {
        :name => "k8s-master2",
        :eth1 => "192.168.100.12",
        :mem => "2048",
        :cpu => "2"
    },
    {
        :name => "k8s-worker1",
        :eth1 => "192.168.100.21",
        :mem => "2048",
        :cpu => "2"
    },
    {
        :name => "k8s-worker2",
        :eth1 => "192.168.100.22",
        :mem => "2048",
        :cpu => "2"
    },
    {
        :name => "k8s-lb",
        :eth1 => "192.168.100.100",
        :mem => "1024",
        :cpu => "1"
    }
]

box = "ubuntu/xenial64"
provision_script = "yes"
synced_folder = "no"

Vagrant.configure("2") do |config|
  config.vm.box = box

# å¾ªç¯è®¾ç½®æ¯å°è™šæ‹Ÿæœº
  boxes.each do |opts|
    config.vm.define opts[:name] do |config|
      # é…ç½® hostname
      config.vm.hostname = opts[:name]

      # é…ç½®å†…å­˜å’ŒCPU
      config.vm.provider "virtualbox" do |v|
        v.customize ["modifyvm", :id, "--memory", opts[:mem]]
        v.customize ["modifyvm", :id, "--cpus", opts[:cpu]]
      end

      # é…ç½® IP
      config.vm.network :private_network, ip: opts[:eth1]

      # è®¾ç½®ç½‘ç»œä»£ç†(éœ€è¦å®‰è£… vagrant-proxyconf æ’ä»¶)

      # è®¾ç½®å¯åŠ¨ååˆå§‹åŒ–è„šæœ¬ï¼Œæ³¨æ„ privileged: trueï¼Œè¡¨ç¤ºä»¥ sudo æ–¹å¼è¿è¡Œ
      if provision_script == "yes"
        if opts[:name] != "k8s-lb"
          config.vm.provision "shell", privileged: true, path: "./setup.sh"
        end
      end

      # è®¾ç½®å®¿ä¸»æœºå…±äº«æ–‡ä»¶å¤¹(éœ€è¦å®‰è£… vagrant-vbguest æ’ä»¶)
      # if synced_folder == "yes"
      #   config.vm.synced_folder ".", "/vagrant", mount_options: ["dmode=774,fmode=775"]
      # end
    end
  end

  if Vagrant.has_plugin?("vagrant-cachier")
      config.cache.scope = :box
  end

  if Vagrant.has_plugin?("vagrant-hostmanager")
		config.hostmanager.enabled = true
		config.hostmanager.manage_host = true
		config.hostmanager.ignore_private_ip = false
		config.hostmanager.include_offline = true
	end

end
```

åˆå§‹åŒ–è„šæœ¬ï¼š

```bash
#!/bin/bash

export DEBIAN_FRONTEND=noninteractive

# ç¦ç”¨é˜²ç«å¢™å¯åŠ¨
sudo ufw disable
sudo iptables -F -t nat && iptables -X -t nat
sudo iptables -P FORWARD ACCEPT

# å…³é—­ selinux
# sudo setenforce 0  # ä¸´æ—¶
# sudo sed -i 's/enforcing/disabled/' /etc/selinux/config  # æ°¸ä¹…

# å…³é—­ swap
sudo swapoff -a  # ä¸´æ—¶
sudo sed -ri 's/.*swap.*/#&/' /etc/fstab

# åŠ è½½æ¨¡å—
sudo modprobe br_netfilter
# éªŒè¯æ¨¡å—æ˜¯å¦ç”Ÿæ•ˆ
lsmod | grep br_netfilter

# æ–°å»ºk8s.confæ–‡ä»¶ï¼Œå¹¶æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼Œè¿™ä¸ªæ˜¯é˜²æ­¢ç”±äº iptables è¢«ç»•è¿‡è€Œå¯¼è‡´æµé‡æ— æ³•æ­£ç¡®è·¯ç”±çš„é—®é¢˜ã€‚
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
vm.swappiness=0
EOF
sudo sysctl --system
ls /proc/sys/net/bridge

# æ—¶é—´åŒæ­¥
sudo apt-get -y install chrony
sudo timedatectl set-timezone Asia/Singapore
sudo systemctl enable chrony
sudo systemctl start chrony

### Install packages to allow apt to use a repository over HTTPS
sudo apt-get update -y && sudo apt-get install -y apt-transport-https ca-certificates curl wget software-properties-common gnupg-agent

# å®‰è£… docker
# Add Docker's official GPG key:
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo apt-key fingerprint 0EBFCD88
# Add the Docker apt repository:
sudo add-apt-repository \
"deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
stable"
# Install Docker CE
sudo apt-get update -y && sudo apt-get install -y docker-ce docker-ce-cli containerd.io
# Restart Docker
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo systemctl enable docker

# å®‰è£… kubeadm, kubectl, kubelet (k8säºŒè¿›åˆ¶æ—¶åˆ™é‡‡ç”¨æ‰‹åŠ¨å®‰è£…æ–¹å¼)
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update -y && sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
sudo systemctl daemon-reload
sudo systemctl restart kubelet
sudo systemctl enable kubelet

sudo mkdir -p /opt/k8s/{bin,cfg,ssl,logs}

cat <<EOF | sudo tee /opt/k8s/env.sh
#!/bin/bash

export ETCDCTL_API=3

# kube-apiserver çš„åå‘ä»£ç†(nginx)åœ°å€ç«¯å£
export KUBE_APISERVER="https://192.168.100.11:6443"

# TLS Bootstrapping ä½¿ç”¨çš„ Tokenï¼Œå¯ä»¥ä½¿ç”¨ head -c 16 /dev/urandom | od -An -t x | tr -d ' ' ç”Ÿæˆ
export BOOTSTRAP_TOKEN="c47ffb939f5ca36231d9e3121a252940"

# Pod ç½‘æ®µ
CLUSTER_CIDR="10.10.0.0/16"

# SERVICEç½‘æ®µ
SERVICE_CIDR="10.0.0.0/24"

# Kubernetes æœåŠ¡ IP (ä¸€èˆ¬æ˜¯ SERVICE_CIDR ä¸­ç¬¬ä¸€ä¸ªIP)
CLUSTER_SERVICE_IP="10.0.0.1"

# é›†ç¾¤ DNS æœåŠ¡ IP (ä» SERVICE_CIDR ä¸­é¢„åˆ†é…)
export CLUSTER_SERVICE_DNS_IP="10.0.0.2"

# é›†ç¾¤ DNS åŸŸåï¼ˆæœ«å°¾ä¸å¸¦ç‚¹å·ï¼‰
export CLUSTER_DNS_DOMAIN="cluster.local"

EOF
sudo chmod a+x /opt/k8s/env.sh

# alias
cat <<EOF | sudo tee -a /home/vagrant/.bashrc
alias k=kubectl
alias kc="kubectl config set-context"
alias kd="kubectl describe"
alias kx="kubectl explain"
# grep: print file name, print line number, recursive, ignore-case
alias g="grep -Hnri --color"

source <(kubectl completion bash)
complete -F __start_kubectl k
export dy="--dry-run=client -o yaml"
EOF

cat > /home/vagrant/.vimrc << EOF
set tabstop=2
set expandtab
set shiftwidth=2
set nu
EOF
```

## åˆ›å»ºé›†ç¾¤

kubeadm åˆ›å»º Kubernetes çš„è¿‡ç¨‹å¦‚ä¸‹ï¼š

![kubeadm](/images/k8s/kubeadm-install.jpeg#center)

-   æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹çš„ç½‘ç»œé…ç½®ï¼Œä¾‹å¦‚ k8s-worker1ï¼š

```bash
vagrant@k8s-worker1:~$ cat /etc/hosts
127.0.0.1	localhost

# The following lines are desirable for IPv6 capable hosts
::1	ip6-localhost	ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
ff02::3	ip6-allhosts
127.0.1.1	ubuntu-xenial	ubuntu-xenial

127.0.2.1 k8s-worker1 k8s-worker1

## vagrant-hostmanager-start
192.168.100.11	k8s-master1

192.168.100.12	k8s-master2

192.168.100.21	k8s-worker1

192.168.100.22	k8s-worker2

192.168.100.100	k8s-lb

## vagrant-hostmanager-end

vagrant@k8s-worker1:~$ ip route show
default via 10.0.2.2 dev enp0s3
10.0.2.0/24 dev enp0s3  proto kernel  scope link  src 10.0.2.15
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown
192.168.100.0/24 dev enp0s8  proto kernel  scope link  src 192.168.100.21
```

-   åˆå§‹åŒ–é›†ç¾¤ <span style="color:orange">k8s\-master1ï¼š192\.168\.100\.11 </span>

```bash
kubeadm init \
    --pod-network-cidr=10.10.0.0/16 \
    --service-cidr=10.0.0.0/24 \
    --apiserver-advertise-address=192.168.100.11
```

é…ç½®å«ä¹‰ï¼š

```md
-   pod-network-cidrï¼špod é€šä¿¡ç½‘ç»œ
-   service-cidrï¼šservice é€šä¿¡ç½‘ç»œ
-   apiserver-advertise-addressï¼škube-apiserver ç«¯å£ï¼Œå¦‚æœæ˜¯é«˜å¯ç”¨é›†ç¾¤ï¼Œè¦æ¢æˆ LBï¼Œè¿™é‡Œåªæ˜¯æœ¬æœº
```

æˆåŠŸåä¼šçœ‹åˆ°ï¼š

```bash
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.100.11:6443 --token virvoc.4q5v8rqvuriyr3wy \
    --discovery-token-ca-cert-hash sha256:ce0f619cf549bef741ebcb8c94f0d7853ea2c11ef7573542920e18ad56f7bf40
```

ä¾ç…§ä¸Šé¢çš„æç¤ºæ‰§è¡Œï¼Œå¹¶æ£€æŸ¥ K8s é›†ç¾¤çš„çŠ¶æ€ï¼š

```bash
vagrant@k8s-master1:~$ mkdir -p $HOME/.kube
vagrant@k8s-master1:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
vagrant@k8s-master1:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

vagrant@k8s-master1:~$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   44m
kube-node-lease   Active   44m
kube-public       Active   44m
kube-system       Active   44m

vagrant@k8s-master1:~$ kubectl get pod --all-namespaces
NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE
kube-system   coredns-f9fd979d6-mvd8f               0/1     Pending   0          43m
kube-system   coredns-f9fd979d6-p899v               0/1     Pending   0          43m
kube-system   etcd-k8s-master1                      1/1     Running   0          44m
kube-system   kube-apiserver-k8s-master1            1/1     Running   0          44m
kube-system   kube-controller-manager-k8s-master1   1/1     Running   0          44m
kube-system   kube-proxy-2pcvd                      1/1     Running   0          43m
kube-system   kube-scheduler-k8s-master1            1/1     Running   0          44m

vagrant@k8s-master1:~$ ls /etc/kubernetes/
admin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf

vagrant@k8s-master1:~$ kubectl get nodes
NAME          STATUS     ROLES    AGE   VERSION
k8s-master1   NotReady   master   53m   v1.19.1
```

ä»ä¸Šé¢å¯ä»¥çœ‹åˆ°æ§åˆ¶é¢æ¿çš„ç»„ä»¶ï¼šetcdï¼Œkube-apiserverï¼Œkube-controller-managerï¼Œkube-scheduler ä»¥åŠæ’ä»¶ CoreDNSï¼Œå‡é‡‡ç”¨é™æ€å®¹å™¨æ¨¡å¼éƒ¨ç½²ï¼Œå…¶éƒ¨ç½²æ¸…å•åœ¨ä¸»æœºçš„/etc/kubernetes/manifests ç›®å½•é‡Œï¼Œkubelet ä¼šè‡ªåŠ¨åŠ è½½æ­¤ç›®å½•å¹¶å¯åŠ¨ podã€‚

k8s-master1 çš„çŠ¶æ€è¿˜æ˜¯ NotReadyï¼Œå› ä¸ºç½‘ç»œè¿˜æ²¡æœ‰éƒ¨ç½²ï¼ŒK8s æœ‰ flannelï¼Œcalico ç­‰ç½‘ç»œé€‰é¡¹ï¼Œé€‰æ‹©éƒ¨ç½² calico ç½‘ç»œã€‚

-   éƒ¨ç½² pod ç½‘ç»œ

```bash
vagrant@k8s-master1:~$ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

vagrant@k8s-master1:~$ kubectl get ds,deploy,pods -n kube-system
NAME                         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/calico-node   1         1         1       1            1           kubernetes.io/os=linux   4m16s
daemonset.apps/kube-proxy    1         1         1       1            1           kubernetes.io/os=linux   76m

NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/calico-kube-controllers   1/1     1            1           4m16s
deployment.apps/coredns                   2/2     2            2           76m

NAME                                          READY   STATUS    RESTARTS   AGE
pod/calico-kube-controllers-c9784d67d-mdtzh   1/1     Running   0          4m15s
pod/calico-node-qrlwg                         1/1     Running   0          4m15s
pod/coredns-f9fd979d6-mvd8f                   1/1     Running   0          75m
pod/coredns-f9fd979d6-p899v                   1/1     Running   0          75m
pod/etcd-k8s-master1                          1/1     Running   0          76m
pod/kube-apiserver-k8s-master1                1/1     Running   0          76m
pod/kube-controller-manager-k8s-master1       1/1     Running   0          76m
pod/kube-proxy-2pcvd                          1/1     Running   0          75m
pod/kube-scheduler-k8s-master1                1/1     Running   0          76m

vagrant@k8s-master1:~$ kubectl get node
NAME          STATUS   ROLES    AGE   VERSION
k8s-master1   Ready    master   81m   v1.19.1
```

k8s-master1 çŠ¶æ€å˜ä¸º Ready

-   åŠ å…¥å·¥ä½œèŠ‚ç‚¹ï¼šåœ¨æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œä¾ç…§ä¸Šè¿°æç¤ºè¿è¡Œ

```bash
kubeadm join 192.168.100.11:6443 --token virvoc.4q5v8rqvuriyr3wy \
    --discovery-token-ca-cert-hash sha256:ce0f619cf549bef741ebcb8c94f0d7853ea2c11ef7573542920e18ad56f7bf40
```

ç»è¿‡ä¸€ç‚¹ç‚¹æ—¶é—´ï¼Œåœ¨ k8s-master1 ä¸Šå¯ä»¥çœ‹åˆ°æ‰€æœ‰èŠ‚ç‚¹éƒ½ Readyï¼š

```bash
vagrant@k8s-master1:~$ kubectl get node
NAME          STATUS   ROLES    AGE   VERSION
k8s-master1   Ready    master   96m   v1.19.1
k8s-worker1   Ready    <none>   82s   v1.19.1
k8s-worker2   Ready    <none>   46s   v1.19.1
```

ğŸ’¡ kubeadm å¯ä»¥æ‰©å±•å•èŠ‚ç‚¹é›†ç¾¤è‡³å¤šèŠ‚ç‚¹ï¼Œå…¶æ‰©å±•æµç¨‹å¦‚ä¸‹ï¼š

![kubeadm join](/images/k8s/kubeadm-join.jpeg#center)

è¿™é‡Œä¸å°è¯•ã€‚

## åº”ç”¨æµ‹è¯•

åˆ›å»ºä¸€ä¸ª Nginx Deploymentï¼š

```bash
vagrant@k8s-master1:~$ kubectl create deployment nginx --image=nginx
deployment.apps/nginx created

vagrant@k8s-master1:~$ kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed

vagrant@k8s-master1:~$ kubectl get pod,svc -o wide
NAME                         READY   STATUS    RESTARTS   AGE     IP             NODE          NOMINATED NODE   READINESS GATES
pod/nginx-6799fc88d8-z42zc   1/1     Running   0          2m55s   10.100.126.1   k8s-worker2   <none>           <none>

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/kubernetes   ClusterIP   10.1.0.1       <none>        443/TCP        141m    <none>
service/nginx        NodePort    10.1.132.185   <none>        80:30869/TCP   2m42s   app=nginx
```

å¯ä»¥çœ‹åˆ°å…¶ä½äº k8s-worker2 èŠ‚ç‚¹ï¼Œè¯¥èŠ‚ç‚¹ IP ä¸º 192.168.100.12ï¼Œç«¯å£æš´éœ²ä¸º 30869ï¼Œå¯ä»¥ä½¿ç”¨æµè§ˆå™¨è®¿é—®è¯¥åœ°å€ã€‚

## åˆ é™¤é›†ç¾¤

-   åˆ é™¤ä¸€ä¸ªå·¥ä½œ/æ§åˆ¶èŠ‚ç‚¹ï¼š

```bash
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
sudo kubeadm reset
kubectl delete node <node name>
```

-   reset ä¸ä¼šé‡ç½® iptables æˆ– IPVSï¼Œéœ€æ‰‹åŠ¨ï¼š

```bash
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
ipvsadm -C
```

-   å¦‚æœä½¿ç”¨å¤–éƒ¨ etcdï¼Œä¹Ÿéœ€è¦ä½¿ç”¨ etcdctl è¿™æ ·çš„å·¥å…·æ‰‹åŠ¨åˆ é™¤

-   åˆ é™¤è½¯ä»¶ï¼š

```bash
sudo apt-get purge kubeadm kubectl kubelet kubernetes-cni kube\*
sudo apt-get autoremove
sudo rm -rf ~/.kube
```
